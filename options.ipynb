{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "A bunch of imports, you don't have to worry about these\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import gymnasium as gym\n",
    "# from gym.wrappers import Monitor\n",
    "import glob\n",
    "import io\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72\n",
      "decoded state\n",
      "[0, 3, 3, 0]\n",
      "500\n",
      "6\n",
      "(72, -1, False, False, {'prob': 1.0, 'action_mask': array([1, 0, 1, 1, 0, 0], dtype=int8)})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adi/miniconda3/envs/rl/lib/python3.11/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.decode to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.decode` for environment variables or `env.get_wrapper_attr('decode')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'+---------+\\n|\\x1b[35mR\\x1b[0m: | :\\x1b[43m \\x1b[0m:G|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|Y| : |\\x1b[34;1mB\\x1b[0m: |\\n+---------+\\n  (North)\\n'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "The environment used here is extremely similar to the openai gym ones.\n",
    "At first glance it might look slightly different. \n",
    "The usual commands we use for our experiments are added to this cell to aid you\n",
    "work using this environment.\n",
    "'''\n",
    "\n",
    "#Setting up the environment\n",
    "env = gym.make('Taxi-v3', render_mode='ansi')\n",
    "state, _ = env.reset()\n",
    "\n",
    "# The state of the environment\n",
    "print(state)\n",
    "\n",
    "print(\"decoded state\")\n",
    "print(list(env.decode(state)))\n",
    "\n",
    "\n",
    "#The number of states in the environment\n",
    "print(env.observation_space.n)\n",
    "\n",
    "#The number of actions in the environment\n",
    "print(env.action_space.n)\n",
    "\n",
    "#Take a step in the environment\n",
    "print(env.step(1))\n",
    "next_state, reward, done, _, _ = env.step(1)\n",
    "\n",
    "#Render the environment\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_options = 10\n",
    "Q_values = np.zeros((env.observation_space.n, total_options))\n",
    "\n",
    "# Softmax function\n",
    "def softmax(Q_values, state, tau):\n",
    "    q_values = Q_values[state]\n",
    "    q_values = q_values / tau\n",
    "    max_q = np.max(q_values)\n",
    "    e = np.exp(q_values - max_q)\n",
    "    dist = e / np.sum(e)\n",
    "    action = np.random.choice(len(dist), p=dist)\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actions:  ['up', 'right', 'down', 'left', 'pickup', 'dropoff']\n",
    "actions = [0, 1, 2, 3, 4, 5]\n",
    "\n",
    "# options: ['toR', 'toG', 'toY', 'toB']\n",
    "options = [6, 7, 8, 9]\n",
    "\n",
    "def execute_option(Q_values, option, state):\n",
    "    optdone = False\n",
    "    optact = 0\n",
    "    \n",
    "    if option == 6:\n",
    "        decoded_state = list(env.decode(state))\n",
    "        if decoded_state[0] == 0 and decoded_state[1] == 0:\n",
    "            optdone = True\n",
    "        optact = softmax(Q_values=Q_values, state=state, tau=1)\n",
    "\n",
    "    elif option == 7:\n",
    "        decoded_state = list(env.decode(state))\n",
    "        if decoded_state[0] == 0 and decoded_state[1] == 4:\n",
    "            optdone = True\n",
    "        optact = softmax(Q_values=Q_values, state=state, tau=1)\n",
    "\n",
    "    elif option == 8:\n",
    "        decoded_state = list(env.decode(state))\n",
    "        if decoded_state[0] == 4 and decoded_state[1] == 0:\n",
    "            optdone = True\n",
    "        optact = softmax(Q_values=Q_values, state=state, tau=1)\n",
    "\n",
    "    elif option == 9:\n",
    "        decoded_state = list(env.decode(state))\n",
    "        if decoded_state[0] == 4 and decoded_state[1] == 3:\n",
    "            optdone = True\n",
    "        optact = softmax(Q_values=Q_values, state=state, tau=1)\n",
    "\n",
    "    return optact, optdone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 16/1000 [00:41<42:54,  2.62s/it] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[89], line 33\u001b[0m\n\u001b[1;32m     31\u001b[0m current_state \u001b[38;5;241m=\u001b[39m state\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m (optdone \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m---> 33\u001b[0m     optact,optdone \u001b[38;5;241m=\u001b[39m \u001b[43mexecute_option\u001b[49m\u001b[43m(\u001b[49m\u001b[43mQ_values\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m     action_to_take \u001b[38;5;241m=\u001b[39m optact\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m action_to_take \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m6\u001b[39m:\n",
      "Cell \u001b[0;32mIn[88], line 21\u001b[0m, in \u001b[0;36mexecute_option\u001b[0;34m(Q_values, option, state)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m decoded_state[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m decoded_state[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m:\n\u001b[1;32m     20\u001b[0m         optdone \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m     optact \u001b[38;5;241m=\u001b[39m \u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mQ_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mQ_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtau\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m option \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m8\u001b[39m:\n\u001b[1;32m     24\u001b[0m     decoded_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(env\u001b[38;5;241m.\u001b[39mdecode(state))\n",
      "Cell \u001b[0;32mIn[87], line 11\u001b[0m, in \u001b[0;36msoftmax\u001b[0;34m(Q_values, state, tau)\u001b[0m\n\u001b[1;32m      9\u001b[0m e \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexp(q_values \u001b[38;5;241m-\u001b[39m max_q)\n\u001b[1;32m     10\u001b[0m dist \u001b[38;5;241m=\u001b[39m e \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39msum(e)\n\u001b[0;32m---> 11\u001b[0m action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(\u001b[38;5;28mlen\u001b[39m(dist), p\u001b[38;5;241m=\u001b[39mdist)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m action\n",
      "File \u001b[0;32mnumpy/random/mtrand.pyx:956\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/rl/lib/python3.11/site-packages/numpy/core/getlimits.py:484\u001b[0m, in \u001b[0;36mfinfo.__new__\u001b[0;34m(cls, dtype)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;124;03mfinfo(dtype)\u001b[39;00m\n\u001b[1;32m    382\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    479\u001b[0m \n\u001b[1;32m    480\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    482\u001b[0m _finfo_cache \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 484\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__new__\u001b[39m(\u001b[38;5;28mcls\u001b[39m, dtype):\n\u001b[1;32m    485\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    486\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_finfo_cache\u001b[38;5;241m.\u001b[39mget(dtype)  \u001b[38;5;66;03m# most common path\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#### SMDP Q-Learning \n",
    "\n",
    "# Add parameters you might need here\n",
    "gamma = 0.9\n",
    "alpha = 0.1\n",
    "update_frequencySMDP = np.zeros((48,6))\n",
    "# Iterate over 1000 episodes\n",
    "for _ in tqdm(range(1000)):\n",
    "    state, _ = env.reset()    \n",
    "    done = False\n",
    "\n",
    "    # While episode is not over\n",
    "    while not done:\n",
    "        \n",
    "        # Choose action        \n",
    "        action = softmax(Q_values, state, tau=1)\n",
    "        # Checking if primitive action\n",
    "        if action < 4:\n",
    "            # Perform regular Q-Learning update for state-action pair\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            Q_values[state, action] += alpha * (reward + gamma * np.max(Q_values[next_state]) - Q_values[state, action])\n",
    "            state = next_state\n",
    "            \n",
    "            \n",
    "        # Checking if action chosen is an option\n",
    "        reward_bar = 0\n",
    "        if action >= 6: # action => Away option\n",
    "            count = 0\n",
    "            optdone = False\n",
    "            optact = 11\n",
    "            current_state = state\n",
    "            while (optdone == False):\n",
    "                optact,optdone = execute_option(Q_values , action, state)\n",
    "                action_to_take = optact\n",
    "                while action_to_take >= 6:\n",
    "                    action_to_take, _ = execute_option(Q_values, action, state)\n",
    "                next_state, reward, done, _, _ = env.step(action_to_take)\n",
    "                reward_bar = reward_bar + gamma**count*reward\n",
    "                count += 1\n",
    "                if optdone == True:\n",
    "                    Q_values[current_state, action] += alpha * (reward_bar - Q_values[current_state, action] + gamma**count * np.max(Q_values[next_state]))\n",
    "                state = next_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
