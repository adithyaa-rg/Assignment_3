{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "A bunch of imports, you don't have to worry about these\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import gymnasium as gym\n",
    "# from gym.wrappers import Monitor\n",
    "import glob\n",
    "import io\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71\n",
      "decoded state\n",
      "[0, 3, 2, 3]\n",
      "500\n",
      "6\n",
      "(71, -1, False, False, {'prob': 1.0, 'action_mask': array([1, 0, 1, 1, 0, 0], dtype=int8)})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adi/miniconda3/envs/rl/lib/python3.11/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.decode to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.decode` for environment variables or `env.get_wrapper_attr('decode')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'+---------+\\n|R: | :\\x1b[43m \\x1b[0m:G|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|\\x1b[34;1mY\\x1b[0m| : |\\x1b[35mB\\x1b[0m: |\\n+---------+\\n  (North)\\n'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "The environment used here is extremely similar to the openai gym ones.\n",
    "At first glance it might look slightly different. \n",
    "The usual commands we use for our experiments are added to this cell to aid you\n",
    "work using this environment.\n",
    "'''\n",
    "\n",
    "#Setting up the environment\n",
    "env = gym.make('Taxi-v3', render_mode='ansi')\n",
    "state, _ = env.reset()\n",
    "\n",
    "# The state of the environment\n",
    "print(state)\n",
    "\n",
    "print(\"decoded state\")\n",
    "print(list(env.decode(state)))\n",
    "\n",
    "\n",
    "#The number of states in the environment\n",
    "print(env.observation_space.n)\n",
    "\n",
    "#The number of actions in the environment\n",
    "print(env.action_space.n)\n",
    "\n",
    "#Take a step in the environment\n",
    "print(env.step(1))\n",
    "next_state, reward, done, _, _ = env.step(1)\n",
    "\n",
    "#Render the environment\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 6)\n"
     ]
    }
   ],
   "source": [
    "total_options = 10\n",
    "Q_values = np.zeros((env.observation_space.n, total_options))\n",
    "print(Q_values.T[:6].T.shape)\n",
    "\n",
    "# Softmax function\n",
    "def softmax(Q_values, state, tau):\n",
    "    q_values = Q_values[state]\n",
    "    q_values = q_values / tau\n",
    "    max_q = np.max(q_values)\n",
    "    e = np.exp(q_values - max_q)\n",
    "    dist = e / np.sum(e)\n",
    "    action = np.random.choice(len(dist), p=dist)\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actions:  ['up', 'right', 'down', 'left', 'pickup', 'dropoff']\n",
    "actions = [0, 1, 2, 3, 4, 5]\n",
    "\n",
    "# options: ['toR', 'toG', 'toY', 'toB']\n",
    "options = [6, 7, 8, 9]\n",
    "\n",
    "def execute_option(Q_values, option, state):\n",
    "    optdone = False\n",
    "    optact = 0\n",
    "    \n",
    "    if option == 6:\n",
    "        decoded_state = list(env.decode(state))\n",
    "        if decoded_state[0] == 0 and decoded_state[1] == 0:\n",
    "            optdone = True\n",
    "        \n",
    "        optact = softmax(Q_values=Q_values.T[:6].T, state=state, tau=1)\n",
    "\n",
    "    elif option == 7:\n",
    "        decoded_state = list(env.decode(state))\n",
    "        if decoded_state[0] == 0 and decoded_state[1] == 4:\n",
    "            optdone = True\n",
    "        optact = softmax(Q_values=Q_values.T[:6].T, state=state, tau=1)\n",
    "\n",
    "    elif option == 8:\n",
    "        decoded_state = list(env.decode(state))\n",
    "        if decoded_state[0] == 4 and decoded_state[1] == 0:\n",
    "            optdone = True\n",
    "        optact = softmax(Q_values=Q_values.T[:6].T, state=state, tau=1)\n",
    "\n",
    "    elif option == 9:\n",
    "        decoded_state = list(env.decode(state))\n",
    "        if decoded_state[0] == 4 and decoded_state[1] == 3:\n",
    "            optdone = True\n",
    "        optact = softmax(Q_values=Q_values.T[:6].T, state=state, tau=1)\n",
    "\n",
    "    return optact, optdone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/1000 [00:07<2:10:20,  7.83s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[104], line 33\u001b[0m\n\u001b[1;32m     31\u001b[0m current_state \u001b[38;5;241m=\u001b[39m state\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m (optdone \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m---> 33\u001b[0m     optact,optdone \u001b[38;5;241m=\u001b[39m \u001b[43mexecute_option\u001b[49m\u001b[43m(\u001b[49m\u001b[43mQ_values\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m     action_to_take \u001b[38;5;241m=\u001b[39m optact\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m action_to_take \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m6\u001b[39m:\n",
      "Cell \u001b[0;32mIn[103], line 31\u001b[0m, in \u001b[0;36mexecute_option\u001b[0;34m(Q_values, option, state)\u001b[0m\n\u001b[1;32m     28\u001b[0m     optact \u001b[38;5;241m=\u001b[39m softmax(Q_values\u001b[38;5;241m=\u001b[39mQ_values\u001b[38;5;241m.\u001b[39mT[:\u001b[38;5;241m6\u001b[39m]\u001b[38;5;241m.\u001b[39mT, state\u001b[38;5;241m=\u001b[39mstate, tau\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m option \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m9\u001b[39m:\n\u001b[0;32m---> 31\u001b[0m     decoded_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m(state))\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m decoded_state[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m decoded_state[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[1;32m     33\u001b[0m         optdone \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/rl/lib/python3.11/site-packages/gymnasium/core.py:311\u001b[0m, in \u001b[0;36mWrapper.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    310\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccessing private attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is prohibited\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 311\u001b[0m \u001b[43mlogger\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwarn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43menv.\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mname\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m to get variables from other wrappers is deprecated and will be removed in v1.0, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    313\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mto get this variable you can do `env.unwrapped.\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mname\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m` for environment variables or `env.get_wrapper_attr(\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mname\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m)` that will search the reminding wrappers.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    314\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, name)\n",
      "File \u001b[0;32m~/miniconda3/envs/rl/lib/python3.11/site-packages/gymnasium/logger.py:55\u001b[0m, in \u001b[0;36mwarn\u001b[0;34m(msg, category, stacklevel, *args)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Raises a warning to the user if the min_level <= WARN.\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \n\u001b[1;32m     48\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;124;03m    stacklevel: The stack level to raise to\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m min_level \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m WARN:\n\u001b[0;32m---> 55\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m     56\u001b[0m         colorize(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWARN: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;250m \u001b[39m\u001b[38;5;241m%\u001b[39m\u001b[38;5;250m \u001b[39margs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myellow\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     57\u001b[0m         category\u001b[38;5;241m=\u001b[39mcategory,\n\u001b[1;32m     58\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     59\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#### SMDP Q-Learning \n",
    "\n",
    "# Add parameters you might need here\n",
    "gamma = 0.9\n",
    "alpha = 0.1\n",
    "update_frequencySMDP = np.zeros((48,6))\n",
    "# Iterate over 1000 episodes\n",
    "for _ in tqdm(range(1000)):\n",
    "    state, _ = env.reset()    \n",
    "    done = False\n",
    "\n",
    "    # While episode is not over\n",
    "    while not done:\n",
    "        \n",
    "        # Choose action        \n",
    "        action = softmax(Q_values, state, tau=1)\n",
    "        # Checking if primitive action\n",
    "        if action < 4:\n",
    "            # Perform regular Q-Learning update for state-action pair\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            Q_values[state, action] += alpha * (reward + gamma * np.max(Q_values[next_state]) - Q_values[state, action])\n",
    "            state = next_state\n",
    "            \n",
    "            \n",
    "        # Checking if action chosen is an option\n",
    "        reward_bar = 0\n",
    "        if action >= 6: # action => Away option\n",
    "            count = 0\n",
    "            optdone = False\n",
    "            optact = 11\n",
    "            current_state = state\n",
    "            while (optdone == False):\n",
    "                optact,optdone = execute_option(Q_values , action, state)\n",
    "\n",
    "                next_state, reward, done, _, _ = env.step(optact)\n",
    "                reward_bar = reward_bar + gamma**count*reward\n",
    "                count += 1\n",
    "                if optdone == True:\n",
    "                    Q_values[current_state, action] += alpha * (reward_bar - Q_values[current_state, action] + gamma**count * np.max(Q_values[next_state]))\n",
    "                state = next_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
