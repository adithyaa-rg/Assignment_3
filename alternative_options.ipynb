{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import gymnasium as gym\n",
    "# from gym.wrappers import Monitor\n",
    "import glob\n",
    "import io\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up the environment\n",
    "env = gym.make('Taxi-v3', render_mode='ansi')\n",
    "state, _ = env.reset()\n",
    "\n",
    "# The state of the environment\n",
    "print(state)\n",
    "\n",
    "print(\"decoded state\")\n",
    "print(list(env.decode(state)))\n",
    "\n",
    "\n",
    "#The number of states in the environment\n",
    "print(env.observation_space.n)\n",
    "\n",
    "#The number of actions in the environment\n",
    "print(env.action_space.n)\n",
    "\n",
    "#Take a step in the environment\n",
    "next_state, reward, done, _, _ = env.step(1)\n",
    "print(\"decoded state\")\n",
    "print(list(env.decode(next_state)))\n",
    "\n",
    "#Render the environment\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Options](https://www.researchgate.net/publication/292208109/figure/fig2/AS:960497125978114@1606011768506/Taxi-problem-and-an-action-hierarchy.gif)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actions:  ['down', 'up', 'right', 'left', 'pickup', 'dropoff']\n",
    "actions = [0, 1, 2, 3, 4, 5]\n",
    "\n",
    "\n",
    "goal_states = [[0, 0], [0, 4], [4, 0], [4, 3]]\n",
    "\n",
    "level_1_options = [0, 1]\n",
    "# highest level options: ['get', 'put']\n",
    "\n",
    "get_put_options = [0, 1, 2, 3, 4]\n",
    "# get options: ['pickup/dropff', 'getR', 'getG', 'getY', 'getB']\n",
    "\n",
    "\n",
    "navigate_options = [0, 1, 2, 3]\n",
    "# navigate options: ['down', 'up', 'right', 'left']\n",
    "Q_level_one_options = np.zeros((500, 2)) # Q-values for level options -> get/put\n",
    "\n",
    "Q_level_two_options = np.zeros((2, 500, len(get_put_options))) # Q-values for level options -> get/put, toR, toG, toY, toB\n",
    "\n",
    "Q_level_three_options = np.zeros((4, 25, len(navigate_options))) # Q-values for level options -> down, up, right, left\n",
    "\n",
    "\n",
    "# Softmax function\n",
    "def softmax(Q, state, tau):\n",
    "    q_values = Q[state]\n",
    "    q_values = q_values / tau\n",
    "    max_q = np.max(q_values)\n",
    "    e = np.exp(q_values - max_q)\n",
    "    dist = e / np.sum(e)\n",
    "    action = np.random.choice(len(dist), p=dist)\n",
    "    return action\n",
    "\n",
    "def epsilon_greedy(Q, state, epsilon):\n",
    "    if random.uniform(0, 1) < epsilon:\n",
    "        action = np.random.randint(0, 4)\n",
    "    else:\n",
    "        action = np.argmax(Q[state])\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_state(env, state):\n",
    "    return list(env.decode(state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### HYPERPARAMETERS ###\n",
    "ALPHA_Q = 0.2\n",
    "ALPHA_OPTIONS = 0.3\n",
    "ALPHA_OPTIONS_1 = 0.4\n",
    "GAMMA = 0.99\n",
    "TAU = 0.1\n",
    "DECAY_CONSTANT = 0.995"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_level_1_selection(env, state, Q_level_one_options, Q_level_two_options, Q_level_three_options, option, policy, TAU):\n",
    "    \"\"\"\n",
    "    Execute the level 1 selection\n",
    "\n",
    "    Args:\n",
    "    env: the environment\n",
    "    state: the current state\n",
    "    level_1_options: the level 1 options\n",
    "    Q_level_two_options: Q-values for level 2 options\n",
    "    Q_level_three_options: Q-values for level 3 options\n",
    "    policy: the policy to use\n",
    "\n",
    "    Returns:\n",
    "    next_state: the next state\n",
    "    reward: the reward\n",
    "    done: is the episode done\n",
    "    Q_level_two_options: updated Q-values for level 2 options\n",
    "    Q_level_three_options: updated Q-values for level 3 options\n",
    "    \"\"\"\n",
    "    \n",
    "    if option < 2:\n",
    "        \"\"\"\n",
    "        Get the Q-values for the level 2 options based on the option chosen\n",
    "        Initialize the reward bar to 0\n",
    "        Initialize the counter to 0\n",
    "        Initialize the current state to the current state\n",
    "        Get the Q-values for the chosen option\n",
    "        Update the Q-values for the chosen option based on the reward and the Q-values for the next state\n",
    "        \"\"\"\n",
    "        optdone = False\n",
    "        optact = 0\n",
    "        counter = 0\n",
    "        reward_bar = 0\n",
    "        chosen_q = Q_level_two_options[option]\n",
    "        steps = 0\n",
    "        while not optdone:\n",
    "            optact = policy(chosen_q, state, TAU)\n",
    "            next_state, reward, done, [Q_level_two_options, Q_level_three_options], pick_drop_flag, steps_level_2 = execute_level_2_options(env, Q_level_two_options, Q_level_three_options, state, option, optact, policy, TAU)\n",
    "            reward_bar += reward * GAMMA**counter\n",
    "            steps += steps_level_2\n",
    "            chosen_q[state][optact] += ALPHA_OPTIONS * (reward + GAMMA * np.max(chosen_q[next_state]) - chosen_q[state][optact])\n",
    "            if pick_drop_flag:\n",
    "                # print(\"Enters here\")\n",
    "                optdone = True\n",
    "                Q_level_one_options[state][option] += ALPHA_Q * (reward_bar - Q_level_one_options[state][option] + GAMMA ** counter * np.max(Q_level_one_options[next_state]))\n",
    "            counter += 1\n",
    "            state = next_state\n",
    "        Q_level_two_options[option] = chosen_q\n",
    "        return next_state, reward_bar, done, [Q_level_one_options, Q_level_two_options, Q_level_three_options], steps\n",
    "    # else:\n",
    "    #     next_state, reward, done, _, _ = env.step(option-2)\n",
    "    #     Q_level_one_options[state][option] += ALPHA_Q * (reward - Q_level_one_options[state][option] + GAMMA * np.max(Q_level_one_options[next_state]))\n",
    "    #     steps = 1\n",
    "    #     return next_state, reward, done, [Q_level_one_options, Q_level_two_options, Q_level_three_options], steps\n",
    "\n",
    "\n",
    "def execute_level_2_options(env, Q_level_two_options, Q_level_three_options, state, option_level_1, option_level_2, policy, TAU):\n",
    "    \"\"\"\n",
    "    Execute the level 2 options\n",
    "\n",
    "    Args:\n",
    "    env: the environment\n",
    "    Q_level_two_options: Q-values for level 2 options\n",
    "    Q_level_three_options: Q-values for level 3 options \n",
    "    state: the current state\n",
    "    option: the selected option\n",
    "    level_1_option: the selected level 1 option\n",
    "    policy: the policy to use\n",
    "\n",
    "    Returns:\n",
    "    next_state: the next state\n",
    "    reward: the reward\n",
    "    done: is the episode done\n",
    "    Q_level_two_options: updated Q-values for level 2 options\n",
    "    Q_level_three_options: updated Q-values for level 3 options\n",
    "    \"\"\"\n",
    "    optdone = False\n",
    "    optact = 0\n",
    "    if option_level_2 == 0: # pickup/dropoff\n",
    "        \"\"\"\n",
    "        Pickup or dropoff\n",
    "        If level 1 option is get, and option is 0, then pickup(4)\n",
    "        If level 1 option is put, and option is 0, then dropoff(5)    \n",
    "        \"\"\"\n",
    "        if option_level_1 == 0:\n",
    "            chosen_action = 4\n",
    "        elif option_level_1 == 1:\n",
    "            chosen_action = 5\n",
    "        steps = 1\n",
    "\n",
    "        \"\"\"\n",
    "        Execute the action if it is pickup or dropoff\n",
    "        Update the Q-values for the level 2 options based on the reward (500, 5)\n",
    "        \"\"\"\n",
    "        next_state, reward, done, _, _ = env.step(chosen_action)\n",
    "        Q_level_two_options[option_level_1][state][option_level_2] +=  ALPHA_OPTIONS * (reward + GAMMA * np.max(Q_level_two_options[option_level_1][next_state]) - Q_level_two_options[option_level_1][state][option_level_2])\n",
    "        \"\"\"\n",
    "        If the next state is the goal state, then the option is done\n",
    "        Returns the next state, reward, done, Q-values for level 2 options, Q-values for level 3 options\n",
    "        \"\"\"\n",
    "        # print(\"Pickup or dropoff\")\n",
    "        return next_state, reward, done, [Q_level_two_options, Q_level_three_options], True, steps\n",
    "    \n",
    "    else: # getR, getG, getY, getB\n",
    "        \"\"\"\n",
    "        If the option is not pickup/dropoff, then it is getR, getG, getY, getB\n",
    "        Get the index of the chosen action\n",
    "        Initialize the reward bar to 0\n",
    "        Initialize the counter to 0\n",
    "        Initialize the current state to the current state\n",
    "        Based on the chosen action, get the Q-values for the chosen action (25, 4)\n",
    "        \"\"\"\n",
    "        reward_bar = 0\n",
    "        chosen_action_index = get_put_options.index(option_level_2) - 1\n",
    "        counter = 0\n",
    "        current_state = state\n",
    "        chosen_q = Q_level_three_options[chosen_action_index]\n",
    "        while not optdone:\n",
    "            \"\"\"\n",
    "            While the option is not done, do the following:\n",
    "            Decode the state\n",
    "            Get the state value\n",
    "            Get the action based on the policy and chosen Q-values(25, 4), state value, and tau \n",
    "            Execute the action\n",
    "            Get the next state value\n",
    "            Update the reward bar\n",
    "            Update the Q-values for the chosen action based on the reward and the Q-values for the next state\n",
    "            \"\"\"\n",
    "            state_value = 5 * decode_state(env, state)[0] + decode_state(env, state)[1]\n",
    "            optact = policy(chosen_q, state_value, TAU)\n",
    "            next_state, reward, done, _, _ = env.step(optact)\n",
    "            next_state_value = 5 * decode_state(env, next_state)[0] + decode_state(env, next_state)[1]\n",
    "            reward_bar += reward*GAMMA**counter\n",
    "            if decode_state(env, next_state)[:2] == goal_states[chosen_action_index]:\n",
    "                \"\"\"\n",
    "                If the next state is the goal state, then the option is done\n",
    "                Update the Q-values for the level 2 options based on the reward bar and the Q-values for the next state based on the option chosen by level 3 options\n",
    "                \"\"\"\n",
    "                # print(\"Goal state reached\")\n",
    "                optdone = True\n",
    "                Q_level_two_options[option_level_1][current_state][option_level_2] += ALPHA_OPTIONS * \\\n",
    "                    (reward_bar - Q_level_two_options[option_level_1][current_state][option_level_2] + \\\n",
    "                     GAMMA ** counter * np.max(Q_level_two_options[option_level_1][next_state]))\n",
    "            GOAL_REWARD = 30 if optdone else 0\n",
    "            chosen_q[state_value][optact] += ALPHA_OPTIONS_1 * (reward + GOAL_REWARD + GAMMA * np.max(chosen_q[next_state_value]) - chosen_q[state_value][optact])\n",
    "            counter += 1\n",
    "            state = next_state\n",
    "        Q_level_three_options[chosen_action_index] = chosen_q\n",
    "        steps = counter\n",
    "        # print(reward_bar)\n",
    "        return next_state, reward_bar, done, [Q_level_two_options, Q_level_three_options], False, steps\n",
    "                \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### SMDP Q-Learning\n",
    "episodes = 10000\n",
    "\n",
    "\n",
    "def SMDP_Q_Learning(env, Q_level_one_options, Q_level_two_options, Q_level_three_options, policy, TAU, DECAY_CONSTANT, episodes):\n",
    "    # Iterate over 1000 episodes\n",
    "    total_rewards = np.zeros((episodes))\n",
    "    for i in tqdm(range(episodes)):\n",
    "        state, _ = env.reset()    \n",
    "        done = False\n",
    "        # While episode is not over\n",
    "        episode_steps = 0\n",
    "        while not done:\n",
    "            option = softmax(Q_level_one_options, state, TAU)\n",
    "            # Choose and execute the level 1 option\n",
    "            next_state, reward, done, value_array, steps = execute_level_1_selection(env, state, Q_level_one_options, Q_level_two_options, Q_level_three_options, option, policy, TAU)\n",
    "            Q_values_level_1, Q_values_level_2, Q_values_level_3 = value_array\n",
    "            state = next_state\n",
    "            total_rewards[i] += reward\n",
    "            TAU = max(0.01, TAU * DECAY_CONSTANT)\n",
    "            episode_steps += steps\n",
    "            # print(episode_steps)\n",
    "\n",
    "    return Q_values_level_1, Q_values_level_2, Q_values_level_3, total_rewards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_level_one_options, Q_level_two_options, Q_level_three_options, total_rewards = SMDP_Q_Learning(env, Q_level_one_options, Q_level_two_options, Q_level_three_options, epsilon_greedy, TAU, DECAY_CONSTANT, episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rewards(rewards):\n",
    "    avg_rew = []\n",
    "    std_dev = []\n",
    "    plt.figure(\"Reward Plot\")\n",
    "    scores_window = deque(maxlen=300)\n",
    "    for reward in rewards:\n",
    "        scores_window.append(reward)\n",
    "        avg_rew.append(np.mean(scores_window))\n",
    "        std_dev.append(np.std(scores_window))\n",
    "    plt.plot(avg_rew)\n",
    "    plt.fill_between(range(len(avg_rew)), np.subtract(avg_rew, std_dev), np.add(avg_rew, std_dev), color='b', alpha=0.1)\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Average Reward')\n",
    "    plt.ylim(-500, 200)  # Set the Y-axis range\n",
    "\n",
    "plot_rewards(total_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(total_rewards[-100:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
